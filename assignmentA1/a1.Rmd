---
title: "Prediction of ozone level in Boston"
author: 
- Lucas Emanuel Resck Domingues^[Escola de Matemática Aplicada]
- Lucas Machado Moschen^[Escola de Matemática Aplicada]
output:
  pdf_document: default
  html_notebook: default
---

```{r echo = F, warning = F, message=F}
library(zoo)
library(dplyr)
library(forecast)
library(tseries)
library(bnstruct)
library(lmtest)
library(ggplot2)
```

## Load and visualize

```{r, echo = F}
bos = read.csv("data/BOS.csv")
plot(bos$Date.Local, bos$O3.Mean, main = 'Daily average level of O3 in Boston', 
                                  xlab = 'Date', ylab = 'Kg/m3')
```

## Methodology 

1. Data treatment: analyse missing and duplicated data. 

2. Compare the daily models: we use rollapply using 2 years + 6 days to predict the day, that is, we use $\{X_t\}_{t-2\cdot365}^t$ to predict $X_{t+7}$. We calcute, for each $t > 2\cdot365 + 6$, $|\hat{X}_{t} - X_t|$ and after calculate $\sum_t |\hat{X}_{t} - X_t|$. We used Decomposition, Regression, Holt-Winters and ARMA. 

3. Compare the weekly models: we use rollapply using 2 years + 4 weeks to predict the week, that is, we use $\{X_t\}_{t-2\cdot52}^t$ to predict $X_{t+4}$. We calcute, for each $t > 2\cdot52 + 4$, $|\hat{X}_{t} - X_t|$ and after calculate $\sum_t |\hat{X}_{t} - X_t|$. We used Decomposition, Regression, Holt-Winters and ARMA. 

4. We compare for the first the case the MAE over the 7 days of the prediction forward. 

5. Compare the best models of each type for the 2 cases in the test data using the same approach. 

Obs.: When applying logarithm, we applied $\log(X_t + 1)$, because there are $0$ values, or almost zero, causing numerical problems. When we compare, however, we do the opposite: $\exp\{\hat{X}_t\} - 1$

## Data treatment

We noticed that some days do not exist in the dataset, for example, the day August 31, 2001 does not have information in the dataset. 

```{r, echo = F}
bos[148:153,]
```

Also, there is duplicated days, as June 9, 2002: 

```{r, echo=F}
day <- as.Date(bos$Date.Local[duplicated(bos$Date.Local)])
bos[bos$Date.Local %in% c(toString(day - 1), toString(day), toString(day + 1)),]
```

The duplicated one is easier to deal, but the missing values are harder. First we calculate the mean value between the duplicated. 

```{r, echo = F}
bos[413, "O3.Mean"] = mean(bos[bos$Date.Local == toString(day),]$O3.Mean)
bos <- distinct(bos, Date.Local, .keep_all = TRUE)
```

The rate of missing values is almost 5% of the dataset. 

```{r, echo = F}
o3 <- zooreg(bos$O3.Mean, order.by = as.Date(as.character(bos$Date.Local), format = "%Y-%m-%d"))
o3.ts <- as.ts(o3)
dates = seq(from = as.Date(time(o3[1])), by = "days", to = as.Date(time(o3[length(o3)])))

print(sum(is.na(o3.ts))/length(o3.ts))
```

So as to solve that problem, we make a knn imputation using the month ($k = 30$)

```{r}
o3.clean <- knn.impute(as.matrix(o3.ts), k = 30)
o3.clean <- as.ts(o3.clean)
plot(o3.clean, main = 'Daily average level of O3 in Boston (after imputation)', 
                xlab = 'Date', ylab = 'Kg/m3')
```

We also separate our data between training and test because of modelling best practices.

```{r}
o3_train = o3.clean[1:(length(o3.clean)[1] - 365),]
o3_test = o3.clean[(length(o3.clean)[1] - 365 + 1):length(o3.clean)[1],]
```

## Models: case 1 

Now we develop some models using the train data.

The metric to compare is the Mean Absolute Error (MAE) in the predictions: 

```{r}
mae <- function(ytrue, ypred)
{
    return(mean(abs(ytrue - ypred)))
}
```

We will use `rollapply` in order to calculate the error, considering the last two years to predict one week forward. 

### Baseline Model

```{r, echo = F}
n <- length(o3_train)
t <- seq(1,n)
size <- 2*365 + 7 + 1
```

We will do the naive forecast to the baseline model. It gets $\hat{X}_{t+7} = X_t$.

```{r baseline, echo=F}
baseline_model_day <- function(t) {
  return(as.numeric(o3_train[t[(size-7)]]))
}

prediction = rollapply(t, width = size, baseline_model_day)
baseline_mae <- mae(o3_train[size:n], prediction)

output = function(mae, title, prediction, data, dates, size) {
  print(mae)
  plot(dates[1:length(data)], data,
       main = title, xlab = "t")
  lines(dates[size:length(data)], prediction, col="red", lwd = 1)
  legend(
    x = dates[1],
    y = 0.07,
    legend = c('Real', 'Prediction'),
    col = c('black', 'red'),
    pch = c('', ''),
    lty = c(1, 1)
  )
}

output(baseline_mae, "Baseline model prediction", prediction, o3_train, dates, size)
```

### Decompose

First of all we make a seasonality test using Kruskal-Wallis. Actually it tests whether samples originate from the same distribution. We can organize it to be samples for each corresponding day. We compare two different frequencies: monthly and yearly. The second one showed the smallest p-value, in particular less than 0.05. For that reason, we will use 365 as the seasonality.  

```{r,echo = F}
freq = 31
g <- rep(c(1:freq), ceiling(n/freq))[1:n]
kruskal.test(o3_train, g = g)
```

```{r,echo = F}
freq = 365
g <- rep(c(1:freq), ceiling(n/freq))[1:n]
kruskal.test(o3_train, g = g)
```

#### Additive model

First we analyse the MAE. 

```{r additive-decompose, echo=F}
stl_add_model_day <- function(t) {
  model <- stl(ts(o3_train[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_add_model_day)
stl_add_mae <- mae(o3_train[size:n], prediction[,7])

output(stl_add_mae, "Additive decompose 7 day forward", prediction[,7], o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], prediction[,i])
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

We also can fit the model using `t.window` and analyse the reminder of the method. 

```{r echo=F}
model <- stl(ts(o3_train, frequency = freq), 
             s.window = "periodic", t.window = 2*365 + 1, robust = T)
plot(model)
```

The ACF and the PACF of the reminder: 

```{r, echo = F, warning = F}
checkresiduals(model$time.series[,'remainder'], lag = 2*freq, lag.max = 2*freq)
```

We see that there are a big spike when $\text{lag} = 365$. It seems not so good for a reminder. We could fit an ARMA model in this reminder yet. We also see a similar normal distribution, but the right tail is a little strange for it. It's a good model, but it can be improved. 

#### Multiplicative model 

First we analyse the MAE using the `rollapply` again. 

```{r multiplicative-decompose, echo=F}
data = log(o3_train + 1)
stl_mul_model_day <- function(t) {
  model <- stl(ts(data[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_mul_model_day)
stl_mul_mae <- mae(o3_train[size:n], exp(prediction[,7]) - 1)

output(stl_mul_mae, "Multiplicative decompose 7 days forward", exp(prediction[,7]) - 1, o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], exp(prediction[,i]) - 1)
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

We also can fit the model using `t.window` and analyse the reminder of the method. 

```{r echo=F}
model <- stl(ts(data, frequency = freq), 
             s.window = "periodic", t.window = 2*365 + 1, robust = T)
plot(model)
```

The ACF and the PACF of the reminder: 

```{r, echo = F, warning=F}
checkresiduals(model$time.series[,'remainder'])
```

We see that there are a big spike when $\text{lag} = 365$. It seems not so good for a reminder. We could fit an ARMA model in this reminder yet. The same problem as before. 

### Regression

We tested for seasonalities, and we settled with 365. So we will fit a regression model with the seasonality dummies. We see the MAE:

```{r regression, echo=F}
Q = factor(c(rep(c(1:365), n/365), c(1:(n%%365))))

regression_model_day <- function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  train = data.frame(
    t = t[1:(size-7)],
    o3_train = o3_train[t[1:(size-7)]],
    Q = Q[t[1:(size-7)]]
  )
  mod = lm(o3_train~t+Q, data = train)
  prediction = predict(mod, data.frame(t=t[c((size - 6):size)], Q=Q[t[c((size - 6):size)]]))
  return(prediction)
}

prediction = rollapply(t, width = size, regression_model_day)
regression_mae <- mae(o3_train[size:n], prediction[,7])

output(regression_mae, "Regression 7 days forward", prediction[,7], o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], prediction[,i])
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

Now we analyse the residuals. We fit a LM model in all training data in order to analyse it.

```{r}
train = data.frame(
  t = t,
  o3_train = o3_train,
  Q = Q
)
mod = lm(o3_train~t+Q, data = train)
checkresiduals(mod, lag = 2*freq, lag.max = 2*freq)
```

As before, wee see spikes in $\text{lag} = 365, 730$. We expect a WN to not have this.

### Holt-Winters

Now we will try Holt-Winters models. In fact, because of apparently seasonality, we will consider complete Holt-Winters models, both additive and multiplicative.

#### Additive

```{r additive-hw, echo=F}
hw_add_model_day = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  # We had many difficulties fitting a Holt-Winters model. So we chose alpha and beta using the whole training dataset (doing little modifications) and we let the HoltWinters function optimize gamma. We found it a reasonable choice.
  mod = HoltWinters(ts(o3_train[t[1:(size-7)]], frequency = 365), 
                    beta = 0.001, gamma = NULL, seasonal = "additive", alpha = 0.02216133,
                    optim.start = c(gamma = 0.2962739))
  prediction = forecast(mod, 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_add_model_day)
hw_add_mae <- mae(o3_train[size:n], prediction[,7])

output(hw_add_mae, "Additive Holt-Winters 7 day forward", prediction[,7], o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], prediction[,i])
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

MAE is kind of good, so is the resulting graph, but we have seem better results. Let's analyse the residuals:

```{r, warning=F}
mod = HoltWinters(ts(o3_train, frequency = 365), seasonal = "additive")
checkresiduals(mod, lag = 2*freq, lag.max = 2*freq)
```

We see the optimization is a huge problem in this case, and we need more background on optimization, when we use `rollapply`. We expect around $0.05*730 = 37$ spikes out of the confidence interval. It seems a little higher than that, but the residuals are pretty similar to the normal distribution. The spike around $\text{lag} = 365$ is really strange, because the model seems not capture this seasonality. 

#### Multiplicative

```{r multiplicative-hw, echo=F}
data = o3_train + 1
hw_mult_model_day = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  # We had many difficulties fitting a Holt-Winters model. So we chose alpha and beta using the whole training dataset (doing little modifications) and we let the HoltWinters function optimize gamma. We found it a reasonable choice.
  mod = HoltWinters(ts(data[t[1:(size-7)]], frequency = 365), alpha = 0.02306014, beta = 0.001, gamma = NULL, seasonal = "multiplicative")
  prediction = forecast(mod, 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_mult_model_day)
hw_mult_mae <- mae(o3_train[size:n], prediction[,7] - 1)

output(hw_mult_mae, "Multiplicative Holt-Winters 7 days forward", prediction[,7] - 1, o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], prediction[,i] - 1)
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

The MAE is not so bad. Let's analyse the residuals of an model fitted in all training data:

```{r, warning = F}
mod = HoltWinters(ts(o3_train+1, frequency = 365), seasonal = "multiplicative")
checkresiduals(mod, lag = 2*freq, lag.max = 2*freq)
```

Same problems as before. We see the optimization is a huge problem in this case, and we need more background on optimization, when we use `rollapply`. We expect around $0.05*730 = 37$ spikes out of the confidence interval. It seems a little higher than that, but the residuals are pretty similar to the normal distribution. 

### ARMA

We can see the ACF and PACF:

```{r, echo = F}
par(mfrow = c(2,2))
acf(o3_train, lag.max = 365, main = 'ACF of the data')
pacf(o3_train, lag.max = 365, main = 'PACF of the data')
acf(o3_train, lag.max = 30, main = 'ACF of the data')
pacf(o3_train, lag.max = 30, main = 'PACF of the data')
```

Based on these graphs, we see both graphs has a exponentially decay, the first after the $q - p = -1$ or $q - p = 0$. In order to identify the model, we will compare the adjusted ARMA models with different $p$ and $q$. First we simply fit it to look at the Akaike Information Criteria (AIC) and the significance of the parameters estimated.   

The AIC measures the goodness of fit and the simplicity of the model into a single statistic. Generally we aim to reduce the AIC. 

$$
AIC = 2k - 2\ln(\hat{L}),
$$
where $k = p + q + 2$ and $\hat{L}$ is the maximum value of the likelihood for the model. 

We tested 

```{r, echo=F}
arma_print <- function(ar, ma){
  model <- arima(o3_train, order = c(ar,0,ma))
  print(paste('Model ARMA(', ar, ',', ma, ')', sep = ''))
  print(paste('AIC = ', model$aic, sep = ''))
  print('p-values')
  print(coeftest(model)[,'Pr(>|z|)'])
  print('')
}
arma_print(1,2)
arma_print(2,1)
arma_print(3,1)
arma_print(3,2)
arma_print(2,3)
```

Considering $\alpha = 0.05$, only the first and third models have all parameters significant. But second and tha last have one or two non significant. Considering that, I will compare the two with minimum AIC, the third (ARMA(3,1)) and the fifth (ARMA(2,3)). 

We see the MAE of the ARIMA(3, 0, 1):

```{r arma-1, echo=F}
arma_model_day <- function(t) {
  model <- arima(o3_train[t[1:(size-7)]], order = c(3,0,1))
  prediction = forecast(model, h = 7)
  return(as.numeric(prediction$mean))
}
prediction = rollapply(t, width = size, arma_model_day)
arma_mae <- mae(o3_train[size:n], prediction[,7])
output(arma_mae, "ARIMA(3,0,1) 7 days forward", prediction[,7], o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], prediction[,i])
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

And the MAE of the ARIMA(2, 0, 3):

```{r arma-2, echo=F, warning = F}
arma_model_day <- function(t) {
  model <- arima(o3_train[t[1:(size-7)]], order = c(2,0,3))
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, arma_model_day)
arma_mae <- mae(o3_train[size:n], prediction[,7])
output(arma_mae, "ARIMA(2,0,3) 7 days forward", prediction[,7], o3_train, dates, size)
```

Next we compare the MAE chan changing the day from 1 to 7. 

```{r, echo = F}
mae_compare <- c(1:7)
for (i in mae_compare) {
  mae_compare[i] = mae(o3_train[(size-7+i):(n-7+i)], prediction[,i])
}
barplot(mae_compare, names.arg = c(1:7), 
        main = 'MAE in the train data for different lags', 
        xlab = 'Lags', ylab = 'MAE')
```

The first model seems a little better. So, let's check the residuals to observe it's problems. 

```{r}
model <- arima(o3_train, order = c(3,0,1))
checkresiduals(model, lag = 2*freq, lag.max = 2*freq)
```

It's interesting to note the ACF has a peak around the 365, so the ARMA model did not seem to capture the seasonality. It would be better to fit an Seasonal ARIMA further. The histogram is pretty similar to normal distribution. The test made analyses if the mean is 0. It may be because the p-value is really small. That's the reason to adapt ARMA with STL, that is, fit an ARMA model in the residuals. 

### Adapting ARMA

The ARMA seems to fit well as we can see so far. However, it's not capturing other caracteristics on the data, as seasonality.  For that reason, we will combine the stl and arma model and extract the best of each one. We will decompose the series in trend and seasonality and in the reminder, we fit an arima model with `auto.arima()`. 

```{r, echo=F}
stl_arma_model_day <- function(t) {
  model <- stlm(ts(o3_train[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction <- forecast(model, h = 7, 
                         forecastfunction=function(x,h,level){
                              fit <- arima(x, order=c(3,0,1), include.mean=FALSE)
                         return(forecast(fit,h=N,level=level))})
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_arma_model_day)
stl_arma_mae <- mae(o3_train[size:n], prediction[,7])
output(stl_arma_mae, "STL + ARIMA prediction", prediction[,7], o3_train, dates, size)
```

However the MAE isn't improved by this model. For that reason, this model was disregarded. 

### Comparing models in the test data. 

We chose some of the best models to compare with the test data. 

```{r, echo = F}
# n - k + 1 = size - 1 => k = n + 2 - size
# we want o3_test(new)[size] = o3_test(old)[size] 
o3_test = c(o3_train[(n+2-size):n], o3_test)
dates = dates[(n+2-size):length(dates)]

# updating n and t
n <- length(o3_test)
t <- c(1:n)
```

```{r baseline-test, echo=F}
baseline_model_day <- function(t) {
  return(as.numeric(o3_test[t[(size-7)]]))
}

prediction = rollapply(t, width = size, baseline_model_day)
baseline_mae <- mae(o3_test[size:n], prediction)
output(baseline_mae, "Baseline prediction", prediction, o3_test, dates = dates, size = size)
```

```{r multiplicative-decompose-test, echo=F}
data = log(o3_test + 1)
stl_mul_model_day <- function(t) {
  model <- stl(ts(data[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_mul_model_day)
stl_mul_mae <- mae(o3_test[size:n], exp(prediction[,7]) - 1)
output(stl_mul_mae, "Multiplicative decompose prediction", exp(prediction[,7]) - 1, o3_test, dates = dates, size = size)
```

```{r regression-test, echo=F}
Q = factor(c(rep(c(1:365), n/365), c(1:(n%%365))))

regression_model_day <- function(t) {
  train = data.frame(
    t = t[1:(size-7)],
    o3_test = o3_test[t[1:(size-7)]],
    Q = Q[t[1:(size-7)]]
  )
  mod = lm(o3_test~t+Q, data = train)
  prediction = predict(mod, data.frame(t=t[size], Q=Q[t[size]]))
  return(prediction)
}

prediction = rollapply(t, width = size, regression_model_day)
regression_mae <- mae(o3_test[size:n], prediction)

output(regression_mae, "Regression prediction", prediction, o3_test, dates = dates, size = size)
```

```{r multiplicative-hw-test, echo=F}
data = o3_test + 1
hw_mult_model_day = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  mod = HoltWinters(ts(data[t[1:(size-7)]], frequency = 365), alpha = 0.02306014, beta = 0.001, gamma = NULL, seasonal = "multiplicative")
  prediction = forecast(mod, 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_mult_model_day)
hw_mult_mae <- mae(o3_test[size:n], prediction[,7] - 1)

output(hw_mult_mae, "Multiplicative Holt-Winters prediction", prediction[,7] - 1, o3_test, dates = dates, size = size)
```


```{r arma-1-test, echo=F}
arma_model_day <- function(t) {
  model <- arima(o3_test[t[1:(size-7)]], order = c(3,0,1))
  prediction = forecast(model, h = 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, arma_model_day)
arma_mae <- mae(o3_test[size:n], prediction[,7])
print('MAE ARIMA(3,0,1)')
output(arma_mae, "ARIMA(3,0,1) prediction", prediction[,7], o3_test, dates = dates, size = size)
```

```{r, echo=FALSE}
models = c('Baseline','STL', 'Regression', 'HW', 'ARIMA')
mae_result <- c(baseline_mae, stl_mul_mae, regression_mae, hw_mult_mae, arma_mae)
mae_all <- data.frame(Model = models, MAE = mae_result)
ggplot(mae_all) + 
  geom_bar( aes(x = Model, y = MAE), stat = 'identity', fill="forestgreen", alpha=0.5) + 
  labs(x = 'Models', y = 'MAE', 
      title = "Comparing model's MAE in test data") + 
  theme(plot.title = element_text(hjust = 0.5))
```

We see that our models performed reasonably, highlighted the ARIMA (in fact ARMA) model.

## Models: case 2 

In case two, we have to aggregate the diary days in a week, starting from the sunday, as requested. So we calculate the mean value in the week to be its representant. The models may be very similar to the previous. We may see less outliers. We also will separate train and test data. The first day in the data is April 1, 2001, a Sunday. So we do not worry about that. 

```{r}
o3_week <- c(1:floor(length(o3.clean)/7))
for(i in seq(1,length(o3.clean)-7, 7)){
  o3_week[ceiling(i/7)] = mean(o3.clean[i:(i+6)])
}
dates = seq(from = as.Date(time(o3[1])), to = as.Date(time(o3[length(o3)])), by = "weeks")[1:782]

plot(ts(o3_week), main = 'Weekly average level of O3 in Boston (after imputation)', 
     xlab = 'Week', ylab = 'Kg/m3')

o3_train_week = o3_week[1:(length(o3_week)[1] - 52)]
o3_test_week = o3_week[-c(1:(length(o3_week)[1] - 52))]
```

### Baseline Model

```{r, echo = F}
n <- length(o3_train_week)
t <- seq(1,n)
size <- 2*52 + 4 + 1
```

We will do the naive forecast to the baseline model.

```{r baseline-week, echo=F}
baseline_model_week <- function(t) {
  return(as.numeric(o3_train_week[t[(size-4)]]))
}

prediction = rollapply(t, width = size, baseline_model_week)
baseline_mae <- mae(o3_train[size:n], prediction)

output_week = function(mae, title, prediction, data, dates, size) {
  print(mae)
  plot(dates[1:length(data)], data,
       main = title, xlab = "t")
  lines(dates[size:length(data)], prediction, col="red", lwd = 1)
  legend(
    x = dates[1],
    y = 0.07,
    legend = c('Real', 'Prediction'),
    col = c('black', 'red'),
    pch = c('', ''),
    lty = c(1, 1)
  )
}

output_week(baseline_mae, "Baseline model prediction", prediction, o3_train_week, dates, size)
```

### Decompose

We now check for seasonality considering monthly (4 records) and yearly (52 records). For seasonality of 52, we see better p-value. In fact, less than 0.05. So we will use 52.

```{r,echo = F}
freq = 4
g <- rep(c(1:freq), ceiling(n/freq))[1:n]
kruskal.test(o3_train_week, g = g)
```

```{r,echo = F}
freq = 52
g <- rep(c(1:freq), ceiling(n/freq))[1:n]
kruskal.test(o3_train_week, g = g)
```

#### Additive model

We see the MAE of the additive decompose model:

```{r additive-decompose-week, echo=F}
stl_add_model_week <- function(t) {
  model <- stl(ts(o3_train_week[t[1:(size-4)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_add_model_week)
stl_add_mae <- mae(o3_train_week[size:n], prediction[,4])

output_week(stl_add_mae, "Additive decompose prediction", prediction[,4], 
            o3_train_week, dates, size)
```

We fit the model using `t.window` and analyse the reminder:

```{r echo=F}
model <- stl(ts(o3_train_week, frequency = freq), 
             s.window = "periodic", t.window = 2*52 + 4, robust = T)
plot(model)
```

The ACF and the PACF of the reminder: 

```{r, echo = F, warning = F}
checkresiduals(model$time.series[,3])
```

We see a big spike when $\text{lag} = 52$. It seems not so good for a reminder.

#### Multiplicative model 

We analyse the MAE:

```{r multiplicative-decompose-week, echo=F}
data = log(o3_train_week + 1)
stl_mul_model_week <- function(t) {
  model <- stl(ts(data[t[1:(size-4)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_mul_model_week)
stl_mul_mae <- mae(o3_train_week[size:n], exp(prediction[,4]) - 1)


output_week(stl_mul_mae, "Multiplicative decompose prediction", exp(prediction[,4]) - 1,
            o3_train_week, dates, size)
```

Using `t.window` and analysing the reminder:

```{r echo=F}
model <- stl(ts(data, frequency = freq), 
             s.window = "periodic", t.window = 2*52 + 1, robust = T)
plot(model)
```

The ACF and the PACF of the reminder: 

```{r, echo = F}
par(mfrow = c(1,2))
acf(model$time.series[,'remainder'], lag.max = size, main = 'ACF Remainder')
pacf(model$time.series[,'remainder'], lag.max = size, main = 'PACF Remainder')
```

We also see the spike when $\text{lag} = 52$. It seems not so good for a reminder. The same as before.

### Regression

We tested for seasonalities, and we settled with 52. So we fit a regression model with the seasonality dummies. We see the MAE:

```{r regression-week, echo=F}
Q = factor(c(rep(c(1:52), n/52), c(1:(n%%52))))

regression_model_week <- function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  train = data.frame(
    t = t[1:(size-4)],
    o3_train_week = o3_train_week[t[1:(size-4)]],
    Q = Q[t[1:(size-4)]]
  )
  mod = lm(o3_train_week~t+Q, data = train)
  prediction = predict(mod, data.frame(t=t[size], Q=Q[t[size]]))
  return(prediction)
}

prediction = rollapply(t, width = size, regression_model_week)
regression_mae <- mae(o3_train_week[size:n], prediction)

output_week(regression_mae, "Regression prediction", prediction, o3_train_week, dates, size)
```

Now we analyse the residuals.

```{r}
train = data.frame(
  t = t,
  o3_train_week = o3_train_week,
  Q = Q
)
mod = lm(o3_train_week~t+Q, data = train)
checkresiduals(mod, lag = 2*freq, lag.max = 2*freq)
```

Wee see spikes in $\text{lag} = 52, 104$. Same as before.

### Holt-Winters

Now we will try Holt-Winters models. We will consider complete Holt-Winters models, those with seasonality.

#### Additive

```{r additive-hw-week, echo=F}
hw_add_model_week = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  # We had many difficulties fitting a Holt-Winters model. So we chose alpha and beta using the whole training dataset (doing little modifications) and we let the HoltWinters function optimize gamma. We found it a reasonable choice.
  mod = HoltWinters(ts(o3_train_week[t[1:(size-4)]], frequency = 52), alpha = 0.03640345, beta = 0.003648801, gamma = NULL, seasonal = "additive")
  prediction = forecast(mod, h=4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_add_model_week)
hw_add_mae <- mae(o3_train_week[size:n], prediction[,4])

output_week(hw_add_mae, "Additive Holt-Winters prediction", prediction[,4], o3_train_week, dates, size)
```

MAE is not so bad, but we have seen better results. Let's analyse the residuals:

```{r, warning=F}
mod = HoltWinters(ts(o3_train_week, frequency = 52), seasonal = "additive")
checkresiduals(mod, lag = 2*freq, lag.max = 2*freq)
```

The residuals are pretty good actually. We see a norm histogram and the spikes above the lines, almost all, and we expected around 5 be higher. So, that model sounds good considering the residuals. 

#### Multiplicative

```{r multiplicative-hw-week, echo=F}
data = o3_train_week + 1
hw_mult_model_week = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  # We had many difficulties fitting a Holt-Winters model. So we chose alpha and beta using the whole training dataset (doing little modifications) and we let the HoltWinters function optimize gamma. We found it a reasonable choice.
  mod = HoltWinters(ts(data[t[1:(size-4)]], frequency = 52), alpha = 0.01237447, beta = 0.003307011, gamma = NULL, seasonal = "multiplicative")
  prediction = forecast(mod, h=4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_mult_model_week)
hw_mult_mae <- mae(o3_train_week[size:n], prediction[,4] - 1)

output_week(hw_mult_mae, "Multiplicative Holt-Winters prediction", 
            prediction[,4] - 1, o3_train_week, dates, size)
```

We see also a reasonable MAE. Let's analyse the residuals:

```{r}
mod = HoltWinters(ts(o3_train_week, frequency = 52), seasonal = "multiplicative")
checkresiduals(mod, lag = 2*freq, lag.max = 2*freq)
```

The same as before, except the fact the the ACF of the remainder seems to be a AR model, what is interesting. 

### ARMA

We can see the ACF and PACF:

```{r, echo = F}
par(mfrow = c(1,2))
acf(o3_train_week, lag.max = 52, main = 'ACF of the data')
pacf(o3_train_week, lag.max = 52, main = 'PACF of the data')
```

Based on these graphs, we see both graphs has a exponentially decay, the first after the $q - p = 1$ or $q - p = 2$. In order to identify the model, we will compare the adjusted ARMA models with different $p$ and $q$. First we simply fit it to look at the Akaike Information Criteria (AIC) and the significance of the parameters estimated.   

```{r, echo=F}
arma_print <- function(ar, ma){
  model <- arima(o3_train_week, order = c(ar,0,ma))
  print(paste('Model ARMA(', ar, ',', ma, ')', sep = ''))
  print(paste('AIC = ', model$aic, sep = ''))
  print('p-values')
  print(coeftest(model)[,'Pr(>|z|)'])
  print('')
}
arma_print(1,2)
arma_print(2,1)
arma_print(3,1)
arma_print(1,3)
arma_print(2,3)
```

The last three models are far the best ones. So we'll pick them out to train our model. However, the ARMA(2,3) could no be used, because of stationary problems. So: 

We see the MAE of the ARMA(1,3):

```{r arma-1-week, echo=F}
arma_model_week <- function(t) {
  model <- arima(o3_train_week[t[1:(size-4)]], order = c(1,0,3))
  prediction = forecast(model, h = 4)
  return(as.numeric(prediction$mean))
}
prediction = rollapply(t, width = size, arma_model_week)
arma_mae <- mae(o3_train_week[size:n], prediction)
output_week(arma_mae, "ARIMA(1,0,3) prediction", prediction[,4], o3_train_week, dates, size)
```

And the MAE of the ARMA(3, 1):

```{r arma-2-week-2, echo=F, warning = F}
arma_model_week <- function(t) {
  model <- arima(o3_train_week[t[1:(size-4)]], order = c(3,0,1))
  prediction = forecast(model, h=4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, arma_model_week)
arma_mae <- mae(o3_train_week[size:n], prediction[,4])
output_week(arma_mae, "ARIMA(3,0,1) prediction", prediction[,4], o3_train_week, dates, size)
```

The second model seems a little better. So, let's check the residuals to observe it's problems. 

```{r}
model <- arima(o3_train_week, order = c(3,0,1))
checkresiduals(model, lag = 2*freq, lag.max = 2*freq)
```

It's interesting to note the ACF has a peak around the 52, so the ARMA model did not seem to capture the seasonality. It would be better to fit an Seasonal ARIMA further. The histogram is pretty similar to normal distribution. The test made analyses if the mean is 0. It may be because the p-value is really small. The ACF also suggests that we include a high order parameter in MA and AR. However, in trying to fit an ARMA(3,2), we struggle with stationarity in AR part. For that reason, the model has it fails and alone it seems that it cannot be improved. 

### Adapting ARMA

The ARMA seems to fit well as we can see so far. However, it's not capturing other caracteristics on the data, as seasonality as mentioned.  For that reason, we will combine the STL and ARMA model and extract the best of each one. We will decompose the series in trend and seasonality and in the reminder, we fit an arima model with `auto.arima()`. 

```{r, echo=F}
stl_arma_model_week <- function(t) {
  model <- stl(ts(o3_train_week[t[1:(size-4)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction <- forecast(model, h = 4, method = 'arima')
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_arma_model_week)
stl_arma_mae <- mae(o3_train_week[size:n], prediction[,4])
output_week(stl_arma_mae, "STL + ARIMA prediction", prediction[,4], o3_train_week, dates, size)
```

It has improved STL and ARMA model error in the predictions! So we will take it forwards. 

### Comparing models in the test data. 

We chose some of the best models to compare with the test data. 

```{r, echo = F}
# n - k + 1 = size - 1 => k = n + 2 - size
# we want o3_test(new)[size] = o3_test(old)[size] 
o3_test_week = c(o3_train_week[(n+2-size):n], o3_test_week)
dates = dates[(n+2-size):length(dates)]

# updating n and t
n <- length(o3_test_week)
t <- c(1:n)
```

```{r baseline-week-test, echo=F}
baseline_model_day <- function(t) {
  return(as.numeric(o3_test_week[t[(size-4)]]))
}

prediction = rollapply(t, width = size, baseline_model_day)
baseline_mae <- mae(o3_test_week[size:n], prediction)
output(baseline_mae, "Baseline prediction", prediction, o3_test_week, dates = dates, size = size)
```

```{r multiplicative-decompose-test-week, echo=F}
data = log(o3_test_week + 1)
stl_mul_model_week <- function(t) {
  model <- stl(ts(data[t[1:(size-4)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_mul_model_week)
stl_mul_mae <- mae(o3_test_week[size:n], exp(prediction[,4]) - 1)
output_week(stl_mul_mae, "Multiplicative decompose prediction", 
            exp(prediction[,4]) - 1, o3_test_week, dates, size)
```

```{r regression-test-week, echo=F}
Q = factor(c(rep(c(1:52), n/52), c(1:(n%%52))))

regression_model_week <- function(t) {
  train = data.frame(
    t = t[1:(size-4)],
    o3_test_week = o3_test_week[t[1:(size-4)]],
    Q = Q[t[1:(size-4)]]
  )
  mod = lm(o3_test_week~t+Q, data = train)
  prediction = predict(mod, data.frame(t=t[size], Q=Q[t[size]]))
  return(prediction)
}

prediction = rollapply(t, width = size, regression_model_week)
regression_mae <- mae(o3_test_week[size:n], prediction)

output_week(regression_mae, "Regression prediction", prediction, 
            o3_test_week, dates, size)
```

```{r multiplicative-hw-test-week, echo=F}
data = o3_test_week + 1
hw_mult_model_week = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  # We had many difficulties fitting a Holt-Winters model. So we chose alpha and beta using the whole training dataset (doing little modifications) and we let the HoltWinters function optimize gamma. We found it a reasonable choice.
  mod = HoltWinters(ts(data[t[1:(size-4)]], frequency = 52), alpha = 0.01237447, beta = 0.003307011, gamma = NULL, seasonal = "multiplicative")
  prediction = forecast(mod, 4)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_mult_model_week)
hw_mult_mae <- mae(o3_test_week[size:n], prediction[,4] - 1)

output_week(hw_mult_mae, "Multiplicative Holt-Winters prediction", 
            prediction[,4] - 1, o3_test_week, dates, size)
```

```{r, echo=F}
stl_arma_model_week <- function(t) {
  model <- stl(ts(o3_test_week[t[1:(size-4)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction <- forecast(model, h = 4, method = 'arima')
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_arma_model_week)
arma_mae <- mae(o3_test_week[size:n], prediction[,4])
output_week(arma_mae, "STL + ARIMA prediction", prediction[,4], o3_test_week, dates, size)
```

Finally, we have this bar graphic to compare the values: 

```{r, echo=FALSE}
models = c('Baseline', 'STL', 'Regression', 'HW', 'ARIMA + STL')
mae_result <- c(baseline_mae, stl_mul_mae, regression_mae, hw_mult_mae, arma_mae)
mae_all <- data.frame(Model = models, MAE = mae_result)
ggplot(mae_all) + 
  geom_bar( aes(x = Model, y = MAE), stat = 'identity', fill="forestgreen", alpha=0.5) + 
  labs(x = 'Models', y = 'MAE', 
      title = "Comparing model's MAE in test data") + 
  theme(plot.title = element_text(hjust = 0.5))
```

It's somewhat impressive that the model with the best performance was the regression. But the other models also did well.