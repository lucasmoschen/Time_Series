---
title: "Prediction of ozone level in Boston"
author: 
- Lucas Emanuel Resck Domingues^[Escola de Matemática Aplicada]
- Lucas Machado Moscheb^[Escola de Matemática Aplicada]
output:
  pdf_document: default
  html_notebook: default
---

```{r echo = F, warning = F, message=F}
library(zoo)
library(dplyr)
library(forecast)
library(tseries)
library(bnstruct)
```

# TO DO 

1. Week model: the same analysis

2. Calcular o erro de quatro modelos nos dados de teste. São eles que vão servir para comparação. 

3. Escrever melhor o texto em algumas transições. 

4. Alguma combinação de modelo? 

## Load and visualize

```{r, echo = F}
bos = read.csv("data/BOS.csv")
plot(bos$Date.Local, bos$O3.Mean, main = 'Daily average level of O3 in Boston', 
                                  xlab = 'Date', ylab = 'Kg/m3')
```

## Data treatment

We noticed that some days do not exist in the dataset, for example, the day August 31, 2001 does not have information in the dataset. 

```{r, echo = F}
bos[148:153,]
```

Also, there is duplicated days, as June 9, 2002: 

```{r, echo=F}
day <- as.Date(bos$Date.Local[duplicated(bos$Date.Local)])
bos[bos$Date.Local %in% c(toString(day - 1), toString(day), toString(day + 1)),]
```

The duplicated one is easier to deal, but the NaN values are harder. First we calculate the mean value between the duplicated. 

```{r, echo = F}
bos[413, "O3.Mean"] = mean(bos[bos$Date.Local == toString(day),]$O3.Mean)
bos <- distinct(bos, Date.Local, .keep_all = TRUE)
```

The rate of NaN values is almost 5% of the dataset. 

```{r, echo = F}
o3 <- zooreg(bos$O3.Mean, order.by = as.Date(as.character(bos$Date.Local), format = "%Y-%m-%d"))
o3.ts <- as.ts(o3)

print(sum(is.na(o3.ts))/length(o3.ts))
```

So as to solve that problem, we make a knn imputation using the month ($k = 30$)

```{r}
o3.clean <- knn.impute(as.matrix(o3.ts), k = 30)
o3.clean <- as.ts(o3.clean)
plot(o3.clean, main = 'Daily average level of O3 in Boston (after imputation)', 
                xlab = 'Date', ylab = 'Kg/m3')
```


```{r, echo= F}
o3_train = o3.clean[1:(length(o3)[1] - 365),]
o3_test = o3.clean[(length(o3)[1] - 365 + 1):length(o3)[1],]
```

## Models: case 1 

Now we develop some models using the train data.

The metric to compare is the Mean Absolute Error (MAE) in the predictions: 

```{r}
mae <- function(ytrue, ypred)
{
    return(mean(abs(ytrue - ypred)))
}
```

We will use `rollyapply` in order to calculate the error, considering the last two years to predict one week forward. 

### Baseline Model

```{r, echo = F}
n <- length(o3_train)
t <- seq(1,n)
size <- 2*365 + 7 + 1
```

We will do the naive forecast to the baseline model. 

```{r baseline, echo=F}
baseline_model_day <- function(t) {
  return(as.numeric(o3_train[(size-7)]))
}

prediction = rollapply(t, width = size, baseline_model_day)
baseline_mae <- mae(o3_train[size:n], prediction)

output = function(mae, title, prediction) {
  print(mae)
  plot(time(o3)[1:n], o3_train,
       main = title, xlab = "t")
  lines(time(o3)[size:n], prediction, col="red", lwd = 3)
  legend(
    x = time(o3)[1],
    y = 0.07,
    legend = c('Real', 'Prediction'),
    col = c('black', 'red'),
    pch = c('', ''),
    lty = c(1, 1)
  )
}

output(baseline_mae, "Baseline model prediction", prediction)
```

### Decompose

First of all we make a seasonality test using Kruskal-Wallis. Actually it tests whether samples originate from the same distribution. We can organize it to be samples for each corresponding day. We compare two different frequencies: monthly and yearly. The second one showed the smallest p-value, in particular less than 0.05. For that reason, we will use 365 in the seasonality.  

```{r,echo = F}
freq = 31
g <- rep(c(1:freq), ceiling(n/freq))[1:n]
kruskal.test(o3_train, g = g)
```

```{r,echo = F}
freq = 365
g <- rep(c(1:freq), ceiling(n/freq))[1:n]
kruskal.test(o3_train, g = g)
```

#### Additive model 

First we analyse the MAE. 

```{r additive-decompose, echo=F}
stl_add_model_day <- function(t) {
  model <- stl(ts(o3_train[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_add_model_day)
stl_add_mae <- mae(o3_train[size:n], prediction[,7])

output(stl_add_mae, "Additive decompose prediction", prediction[,7])
```

We also can fit the model using `t.window` and analyse the reminder of the method. 

```{r echo=F}
model <- stl(ts(o3_train, frequency = freq), 
             s.window = "periodic", t.window = 2*365 + 1, robust = T)
plot(model)
```

The ACF and the PACF of the reminder: 

```{r, echo = F}
par(mfrow = c(1,2))
acf(model$time.series[,'remainder'], lag.max = size, main = 'ACF Remainder')
pacf(model$time.series[,'remainder'], lag.max = size, main = 'PACF Remainder')
```

We see that there are a big spike when $\text{lag} = 365$. It seems not so good for a reminder. We could fit an ARMA model in this reminder yet. 

#### Multiplicative model 

First we analyse the MAE. 

```{r multiplicative-decompose, echo=F}
data = log(o3_train + 1)
stl_mul_model_day <- function(t) {
  model <- stl(ts(data[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_mul_model_day)
stl_mul_mae <- mae(o3_train[size:n], exp(prediction[,7]) - 1)


output(stl_mul_mae, "Multiplicative decompose prediction", exp(prediction[,7]) - 1)
```

We also can fit the model using `t.window` and analyse the reminder of the method. 

```{r echo=F}
model <- stl(ts(data, frequency = freq), 
             s.window = "periodic", t.window = 2*365 + 1, robust = T)
plot(model)
```

The ACF and the PACF of the reminder: 

```{r, echo = F}
par(mfrow = c(1,2))
acf(model$time.series[,'remainder'], lag.max = size, main = 'ACF Remainder')
pacf(model$time.series[,'remainder'], lag.max = size, main = 'PACF Remainder')
```

We see that there are a big spike when $\text{lag} = 365$. It seems not so good for a reminder. We could fit an ARMA model in this reminder yet. The same problem as before. 

### Regression

In this case 1, with daily records, it's reasonable seasonality of one year.

```{r regression, echo=F}
Q = factor(c(rep(c(1:365), n/365), c(1:(n%%365))))

regression_model_day <- function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  train = data.frame(
    t = t[1:(size-7)],
    o3_train = o3_train[t[1:(size-7)]],
    Q = Q[t[1:(size-7)]]
  )
  mod = lm(o3_train~t+Q, data = train)
  prediction = predict(mod, data.frame(t=t[size], Q=Q[t[size]]))
  return(prediction)
}

prediction = rollapply(t, width = size, regression_model_day)
regression_mae <- mae(o3_train[size:n], prediction)

output(regression_mae, "Regression prediction", prediction)
```

Now we analyse the residuals. The analysed residuals will be those from each model in the rolling window. It's valid because every model has (the assumption of) white noise with the same variance. Let's see the ACF and PACF:

```{r, echo = F}
par(mfrow = c(1,2))
acf(o3_train[size:n]-prediction, lag.max = size, main = 'ACF Remainder')
pacf(o3_train[size:n]-prediction, lag.max = size, main = 'PACF Remainder')
```

As before, wee see spikes in $\text{lag} = 365, 730$. We expect a WN to not have this.

### Holt-Winters

Now we will try Holt-Winters models. In fact, because of apparently seasonality, we will consider complete Holt-Winters models, both additive and multiplicative.

#### Additive

```{r additive-hw, echo=F}
hw_add_model_day = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  mod = HoltWinters(ts(o3_train[t[1:(size-7)]], frequency = 365), beta = T, gamma = T, seasonal = "additive")
  prediction = forecast(mod, 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_add_model_day)
hw_add_mae <- mae(o3_train[size:n], prediction[,7])

output(hw_add_mae, "Additive Holt-Winters prediction", prediction[,7])
```

MAE is not so good. We have seem better. Let's analyse the residuals:

```{r, echo = F}
par(mfrow = c(1,2))
acf(o3_train[size:n]-prediction[,7], lag.max = size, main = 'ACF Remainder')
pacf(o3_train[size:n]-prediction[,7], lag.max = size, main = 'PACF Remainder')
```

We see the same problems as before: high correlated $\text{lag}=365$, evidence of this not beeing a WN.

#### Multiplicative

```{r multiplicative-hw, echo=F}
data = o3_train + 1
hw_mult_model_day = function(t) {
  # if (t[1] %% 100 == 0) {
  #   print(t[1]/n)
  # }
  mod = HoltWinters(ts(data[t[1:(size-7)]], frequency = 365), beta = T, gamma = T, seasonal = "multiplicative")
  prediction = forecast(mod, 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, hw_mult_model_day)
hw_mult_mae <- mae(o3_train[size:n], prediction[,7] - 1)

output(hw_mult_mae, "Multiplicative Holt-Winters prediction", prediction[,7] - 1)
```

We see also a not so good MAE. Let's analyse the residuals:

```{r, echo = F}
par(mfrow = c(1,2))
acf(o3_train[size:n]-prediction[,7], lag.max = size, main = 'ACF Remainder')
pacf(o3_train[size:n]-prediction[,7], lag.max = size, main = 'PACF Remainder')
```

Same problems.

### ARMA

We can see the ACF and PACF:

```{r, echo = F}
par(mfrow = c(2,2))
acf(o3_train, lag.max = 365, main = 'ACF of the data')
pacf(o3_train, lag.max = 365, main = 'PACF of the data')
acf(o3_train, lag.max = 30, main = 'ACF of the data')
pacf(o3_train, lag.max = 30, main = 'PACF of the data')
```

Based on these graphs, we see both graphs has a exponentially decay, the first after the $p - q = 1$ or $p - q = 2$. In order to identify the model, we will compare the adjusted ARMA models with different $p$ and $q$. First we simply fit it to look at the Akaike Information Criteria (AIC) and the significance of the parameters estimated.   

The AIC measures the goodness of fit and the simplicity of the model into a single statistic. Generally we aim to reduce the AIC. 

$$
AIC = 2k - 2\ln(\hat{L}),
$$
where $k = p + q + 2$ and $\hat{L}$ is the maximum value of the likelihood for the model. 

```{r, echo=F}
summary(arma(o3_train, order = c(2,1)))
summary(arma(o3_train, order = c(3,1)))
summary(arma(o3_train, order = c(3,2)))
summary(arma(o3_train, order = c(4,3)))
summary(arma(o3_train, order = c(4,2)))
```

First we see that in the last two model, there is no statistical significance in the major part of the parameters, so we'll no consider it. The first, second and third models have significant parameters and similar AIC. In especial the 2° and 3° has the smallest AIC. So we will compare them both. 

```{r arma-1, echo=F}
arma_model_day <- function(t) {
  model <- arima(o3_train[t[1:(size-7)]], order = c(3,0,1))
  prediction = forecast(model, h = 7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, arma_model_day)
arma_mae <- mae(o3_train[size:n], prediction[,7])
print('MAE ARIMA(3,0,1)')
output(arma_mae, "ARIMA(3,0,1) prediction", prediction[,7])
```

```{r arma-2, echo=F, warning = F}
arma_model_day <- function(t) {
  model <- arima(o3_train[t[1:(size-7)]], order = c(3,0,2))
  prediction = forecast(model, h=7)
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, arma_model_day)
arma_mae <- mae(o3_train[size:n], prediction[,7])
print('MAE ARIMA(3,0,2)')
output(arma_mae, "ARIMA(3,0,2) prediction", prediction[,7])
```

The first model seems a little better. 

### Adapting ARMA

The ARMA seems to fit well as we can see so far. However, it's not capturing other caracteristics on the data, as seasonality.  For that reason, we will combine the stl and arma model and extract the best of each one. We will decompose the series in trend and seasonality and in the reminder, we fit an arima model with `auto.arima()`. 

```{r, echo=F}
stl_arma_model_day <- function(t) {
  model <- stl(ts(o3_train[t[-c((size-6):size)]], frequency = freq), 
               s.window = "periodic", robust = T)
  prediction <- forecast(model, h = 7, method = 'arima')
  return(as.numeric(prediction$mean))
}

prediction = rollapply(t, width = size, stl_arma_model_day)
stl_arma_mae <- mae(o3_train[size:n], prediction[,7])
output(stl_arma_mae, "STL + ARIMA prediction", prediction[,7])
```

However the MAE doesn't improved the model. For that reason, this model was disregarded. 

### Comparando os modelos na base de testes. 



## Models: case 2 

In case two, we have to aggregate the diary days in a week, starting from the sunday, as requested. So we calculate the mean value in the week to be its representant. The models may be very similar to the previous. We may see less outliers. We also will separate train and test data. The first day in the data is April 1, 2001, a Sunday. SO we do not worry about that. 

```{r}
o3_week <- c(1:floor(length(o3.clean)/7))
for(i in seq(1,length(o3.clean)-7, 7)){
  o3_week[ceiling(i/7)] = mean(o3.clean[i:(i+6)])
}

plot(ts(o3_week), main = 'Weekly average level of O3 in Boston (after imputation)', 
     xlab = 'Week', ylab = 'Kg/m3')

o3_train_week = o3_week[1:(length(o3_week)[1] - 52)]
o3_test_week = o3_week[-c(1:(length(o3_week)[1] - 52))]
```



