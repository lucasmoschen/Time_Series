---
title: "Modelos SARIMA e Previsões"
author: "Lucas Domingues e Lucas Moschen"
date: \today
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Questões Teóricas 

## Questão 6 (Capítulo 9)

Se $Y \sim N\left(\mu, \sigma^2\right)$, então $X$ tal que $\log X = Y$ terá uma distribuição log-normal com $E[X] = e^{\mu + \sigma^2/2}$ e $Var[X] = e^{2\mu + \sigma^2}\left(e^{\sigma^2} - 1\right)$. Baseado nessa definição, se $Y_t = \log Z_t$ e $Y_t$ é Gaussiano, então 
$$
\hat{Z}_t(h) = \exp\left\{\hat{Y}_t(h) + \frac{1}{2}V_y(h)\right\}
$$
$$
V_Z(h) = \exp\left\{2\hat{Y}_t(h) + V_y(h)\right\}\left[\exp\{V_y(h)\} - 1\right]
$$
**Resposta:** Sabemos que a distribuição de $Y_{t+h}$ em um processo Gaussiano é $N\left(\hat{Y}_t(h), V_Y(h)\right)$, onde 
$$\hat{Y}_t(h) = E[Y_{t+h}|Y_t, Y_{t-1}, ...]$$ 
Como $Z_{t+h} = \log Y_{t+h}$, pelo enunciado, a distribuição de $Z_{t+h}|Z_t, Z_{t-1}, ..$ é uma log-normal, tal que: 
$$
\hat{Z}_t(h) = E[Z_{t+h}|Z_t, Z_{t-1},...] = e^{\hat{Y}_t(h) + \frac{1}{2}V_Y(h)}
$$
$$
V_Z(h) = e^{2\hat{Y}_t(h)  + V_Y(h)}\left(e^{V_Y(h)} - 1\right)
$$

utilizando os valores do valor esperado e variância descritos. Isso demonstra a relação.

## Questão 8 (Capítulo 9)

Considere o problema de encontrar a previsão linear ótima (erro quadrático médio mínimo) de um processo estacionário de média zero, $\{Z_t\}$, baseado em um número finito de observações, $Z_t, ..., Z_{t-r}$. Em resumo, queremos encontrar os coeficientes $a_i$ na fórmula de previsão
$$
\hat{Z}_t(h) = a_0Z_t + a_1Z_{t-1}+...+a_rZ_{t-r}
$$
que fornecem erro quadrático médio mínimo. 

(a) Mostre que 
$$
\begin{split}
E\left[(Z_{t+h} - \hat{Z}_t(h))^2\right] &= \gamma_0 - 2\sum_{i=0}^r a_i\gamma_{i+h} + \sum_{i=0}^r \sum_{j=0}^r a_ia_j\gamma_{i-j} \\
&= \gamma_0 - 2\boldsymbol{a}' \gamma_r(h) + \boldsymbol{a}'\Gamma_{r+1}\boldsymbol{a},
\end{split}
$$
onde 
$$
\Gamma_{r+1} = 
\begin{bmatrix}
\gamma_0 & \gamma_1 & ... & \gamma_r \\
\gamma_1 & \gamma_0 & ... & \gamma_{r-1} \\
... & ... & ... & ... \\
\gamma_r & \gamma_{r-1} & ... & \gamma_0
\end{bmatrix}, 
\gamma_r(h) = 
\begin{bmatrix}
\gamma_h \\
\gamma_{h+1} \\
... \\
\gamma_{h+r}
\end{bmatrix}, 
\boldsymbol{a} = 
\begin{bmatrix}
a_0 \\
a_1 \\
... \\
a_r
\end{bmatrix}
$$

**Resposta**: Temos a seguinte expressão:
$$
\begin{split}
E\left[(Z_{t+h} - \hat{Z}_t(h))^2\right] &= E\left[\left(Z_{t+h} - \sum_{i=0}^r a_iZ_{t-i}\right)^2\right] \\
&= E\left[Z_{t+h}^2  - 2Z_{t+h}\sum_{i=0}^r a_iZ_{t-i} + \left(\sum_{i=0}^r a_i Z_{t-i}\right)^2\right] \\
&= E[Z_{t+h}^2] - 2\sum_{i=0}^r a_i E[Z_{t+h}Z_{t-i}] + \sum_{i=0}^r\sum_{j=0}^r a_i a_j E[Z_{t-i}Z_{t-j}] \\
&= \gamma_0 - 2\sum_{i=0}^r a_i\gamma_{h+i} + \sum_{i=0}\sum_{j=0}a_ia_j\gamma_{i-j} \\
&= \gamma_0 - 2\boldsymbol{a}'\gamma_r(h) + \boldsymbol{a}'\Gamma_{r+1}\boldsymbol{a}
\end{split}
$$
onde a última igualdade vem da definição dada pelo enunciado e usando a notação de produto matricial. 

(b) Encontre os $a_i$ que minimizam o EQM e mostre que as equações resultantes são $\Gamma_{r+1}\boldsymbol{a} = \gamma_r(h)$. 

**Resposta:** Queremos minimizar a expressão 
$$
\gamma_0 - 2\boldsymbol{a}^T\gamma_r(h) + \boldsymbol{a}^T\Gamma_{r+1}\boldsymbol{a}
$$
Equivalente a minimizar
$$
f(\boldsymbol{a}) = - 2\boldsymbol{a}^T\gamma_r(h) + \boldsymbol{a}^T\Gamma_{r+1}\boldsymbol{a}
$$
Para isso, fazemos 
$$
\nabla f(\boldsymbol{a}) = - 2\gamma_r(h) + 2\Gamma_{r+1}\boldsymbol{a} = 0 \implies \Gamma_{r+1}\boldsymbol{a} = \gamma_r(h)
$$
Vamos verificar a segunda derivada
$$
\text{Hess}(f(a)) = 2\Gamma_{r+1}
$$
Como a matriz de covariância é positiva semi-definida e, nesse caso, definida, temos um mínimo local quando
$$
\Gamma_{r+1}\boldsymbol{a} = \gamma_r(h),
$$
como queríamos provar. 

# Série Consumo - Questão 3 (Capítulo 10)

Usando um programa de computador apropriado, obtenha as autocorrelações estimadas para $Z_t, \Delta Z_t, \Delta_4 Z_t, \Delta \Delta_4 Z_t$, sendo $Z_t$ a série de consumo de gasolina.

```{r, echo=F, warning=F, message=F}
library(readxl)
library(tseries)
library(fpp2)

consumo <- ts(read_excel("CONSUMO.XLS")$consumo, start = c(1984), frequency = 12)
```

```{r plot-consumo, echo=F}
autoplot(consumo, main = "Consumo trimestral de gasolina na Califórnia", 
                  xlab = 'Tempo (trimestre)', 
                  ylab = 'milhões de galões')
```

(a) O que você pode observar nas autocorrelações de $Z_t$? 

```{r, echo=F}
x <- acf(consumo, lag.max = 48, plot = F)
x$lag <- x$lag * 12
plot(x, main = 'Autocorrelação Zt', 
        xlab="Lag (meses)")
```

Podemos observar um decaimento exponencial no gráfico acima que é não significante apenas após o lag 25. Também podemos observar que os lags 12, 23 e 36 tem picos fora do comum que indicam sazonalidade. 

(b) A mesma pergunta para $\Delta Z_t$.

```{r, echo=F}

x <- acf(diff(consumo), lag.max = 48, plot = F)
x$lag <- x$lag * 12
plot(x, main = 'Autocorrelação Zt', 
        xlab="Lag (meses)")
```

**Resposta:** Podemos observar que grande parte da ACF foi anulada, apenas os lags que indicam uma diferença dos picos de sazonalidade, e os próprios picos. 

(c) Qual das séries você consideraria estacionária?

**Resposta:** Não consideramos nenhuma das séries estacionária. Quando fizemos a primeira diferença, vários lags desapareceram, o que indica uma uma tendência na série original. Porém a segunda série também apresenta lags nos períodos 12, 24, 36 e 48, o que indica a sazonalidade que já havíamos notado. 

(d) Utilizando um programa de identificação, sugira um ou mais modelos adequados para a série; obtenha as estimativas preliminares para os parâmetros.

**Resposta:** Vamos utilizar a função `auto.arima` para identificar o modelo.

```{r, echo=F}
id <- auto.arima(consumo)
summary(id)
```

Vemos que o programa identifica um ARIMA(1,0,1)(0,1,1)[12]. Além disso, ele mostra as estimativas preliminares dos coeficientes.

Também podemos fazer uma identificação de um modelo alternativo feito artesanalmente. Primeiro fazemos uma transformação Box-Cox com o parâmetro $\lambda$ ótimo. Depois podemos fazer um teste de estacionariedade. Porém o teste ADF acusa que não rejeitamos a hipótese nula (de não estacionariedade) a nível 0,05.

```{r, echo=F}
lambda <- BoxCox.lambda(consumo)
consumo.bc <- BoxCox(consumo, lambda)

adf.test(consumo.bc)
```

Portanto, fazemos uma diferença e refazemos o teste.

```{r, echo=F}
consumo.diff = diff(consumo.bc) 
adf.test(consumo.diff)
```

Nesse caso possuimos a estacionaridade desejada. Como ainda temos a sazonalidade já observada, também fazemos a diferença sazonal e, assim: 

```{r, echo=F}
consumo.diff = diff(consumo.diff, 12)
ggtsdisplay(consumo.diff)
```

Observe que a ACF (morte após lag 12) + PACF (decaimento exponencial) nos lags da sazonalidade indicam um componente MA sazonal. Vamos utilizar os critérios de informação para definir o modelo final. Os valores máximos de $p$ e $q$ serão 3 porque tanto a ACF quando a PACF parecem morrer cedo. 

```{r, echo = F}
ARMA.res <- data.frame()
## valor máximo de p,q.
K <- 3
L <- K
for (p in 0:K) {
    for (q in 0:K) {
        model <- Arima(y = consumo,  
                        order = c(p,1,q), 
                        seasonal = c(0,1,1), 
                        lambda = "auto")
        ARMA.res <- rbind(ARMA.res, c(p,q,model$aic, model$bic, model$aicc))
    }
}
names(ARMA.res) = c('p', 'q','AIC', 'BIC', 'AICc')
print('Modelo com menor AIC')
print(ARMA.res[order(ARMA.res$AIC)[1],])
print('Modelo com menor AICc')
print(ARMA.res[order(ARMA.res$AICc)[1],])
print('Modelo com menor BIC')
print(ARMA.res[order(ARMA.res$BIC)[1],])
```

Em particular, as três informações escolheram o modelo ARIMA(2,1,0)(0,1,1)[12]. Por isso, esse é nosso modelo alternativo.

(e) Obtenha as estimativas finais para os parâmetros do(s) modelo(s) através de um programa de estimação; verifique se o(s) modelo(s) é (são) adequado(s).

**Resposta:** Será utilizada a função `Arima`, que estima os parâmetros do modelo de dada ordem. Além disso, ela permite incluir o parâmetro `lambda` para a transformação de Box-Cox.

Vejamos o resultado da estimação para um modelo ARIMA(1,0,1)(0,1,1)[12] com parâmetro `lambda` escolhido automaticamente:

```{r, echo=F}
est <- Arima(consumo, order = c(1,0,1), seasonal =  c(0,1,1), lambda = "auto")
summary(est)
```

Observamos que as estimativas atuais dos parâmetros são próximas daquelas preliminares (do exercício anterior). O parâmetro `lambda` também é estimado.

Vamos analisar os resíduos, através de seu gráfico, sua ACF e seu histograma:

```{r, echo=F}
checkresiduals(est)
```

O teste de Ljung-Box nos resíduos nos dá boas notícias: não rejeitamos a hipótese de dados não correlacionados. Observamos que a ACF nos indica que os resíduos se comportam como esperaríamos de um ruído gaussiano. Todavia, o teste jarque.bera rejeita a hipótese nula de que a assimetria e o excesso de curtose são nulos. Isso nos dá evidência para a não normalidade.  

```{r, echo=F}
jarque.bera.test(est$residuals)
```


Vejamos o resultado de `ggtsdisplay` para os resíduos:

```{r, echo=F}
ggtsdisplay(est$residuals)
```

A PACF também se comporta dentro do intervalo de confiança para que consideremos o resíduo como um ruído branco. 

Para o modelo alternativo ARIMA(2,1,0)(0,1,1)[12], façamos o mesmo processo. 

```{r, echo=F}
est2 <- Arima(consumo, order = c(2,1,0), seasonal =  c(0,1,1), lambda = "auto")
summary(est2)
```

Observamos que alguns indicadores de erros nos mostram que o modelo é pior: tem maior erro média, maior raiz do erro quadrático e erro percentual, mas tem menor MAE, MAPE, MASE e ACF1. 

```{r, echo=F}
checkresiduals(est2)
```

A ACF dos resíduis está particularmente boa! Além disso o teste de Ljung-Box não rejeita a hipótese nula de que não existe correlação, o que é uma boa notícia. Vamos ver o Jarque-Bera.

```{r, echo=F}
jarque.bera.test(est2$residuals)
```
O teste de Jarque Bera também rejeita a hipótese nula, o que não nos dá muito \textit{insight} sobre o modelo ideial. 

Vamos utilizar o primeiro modelo para previsão, dado que obteve menor RMSE. 

(f) Obtenha previsões para 1974 utilizando o(s) modelo(s) estimado(s).

**Resposta:** O ano 1974 é no passado. A última observação é de outubro de 1996. Vamos prever 12 meses à frente utilizando nosso modelo e também mostrando os intervalos de confiança:

```{r, echo=F}
autoplot(forecast(est, h=12, level = c(80, 90, 95)))
```

Visualmente, isso parece um tanto razoável.

# Série de serviços da Austrália 

Essa série representa o gasto mensal total em serviços de cafés, restaurantes e comida para levar na Austrália em bilhões de dólares. Vamos visualizar a série:

```{r visualize-serie-auscafe, echo=F}
autoplot(auscafe, main = 'Gasto em cafés, restaurantes e comidas para levar na Austrália', 
                  ylab = '$ bilhões', 
                  xlab = 'Tempo')
```

## Estabilização da variância

Observamos que a variância está aumentando, então é necessário estabilizá-la.
Vamos utilizar a função `BoxCox`, que calcula automaticamente o parâmetro ótimo `lambda`.
Vejamos o resultado:

```{r, echo=F}
lambda <- BoxCox.lambda(auscafe)
bc_auscafe <- BoxCox(auscafe, lambda)
autoplot(bc_auscafe, main = 'Gasto em cafés, restaurantes e comidas para levar na Austrália (var. estabilizada)',
         ylab = '$ bilhões',
         xlab = 'Tempo') + 
annotate("text", x=1985, y=1.3, label= paste('lambda = ', round(lambda, digits = 3)))
```

## Estacionariedade e diferenciação

Verifiquemos agora estacionariedade e necessidade de diferenciação.
Inicialmente, utilizamos o teste ADF para a verificação da estacionariedade:

```{r, echo = F}
adf.test(bc_auscafe)
```

Não rejeitamos a hipótese de não estacionariedade, logo decidimos por uma diferenciação. Diferenciando e testando:

```{r, echo = F}
d_bc_auscafe = diff(bc_auscafe, 1)
adf.test(d_bc_auscafe)
```

Agora, sim, temos um modelo estacionário.
Como estamos trabalhando com modelos SARIMA, verificamos também a necessidade de uma diferenciação
sazonal. Verificamos isso com os gráficos da ACF e da PACF:

```{r, echo=F}
par(mfrow = c(1,2))
acf(d_bc_auscafe)
pacf(d_bc_auscafe)
```

A presença de sazonalidade no lag 12 (e seus múltiplos e divisores) é clara,
tanto nos gráficos da ACF e PACF, quanto no próprio gráfico da série. Decidimos portanto por realizar uma
diferenciação sazonal de lag 12.

```{r, echo = F}
dd_bc_auscafe = diff(d_bc_auscafe, 12)
adf.test(dd_bc_auscafe)
```

Ainda mantemos a estacionariedade!

```{r, echo=F}
par(mfrow = c(1,2))
acf(dd_bc_auscafe)
pacf(dd_bc_auscafe)
```

Os picos diminuíram de forma considerável.
Os picos na ACF (1, 12 e 23) indicam um termo MA(1) (corte depois de $q > 1$) e um termo sazonal MA(1) (\textit{spike} em lag 12). Os termos autoregressivos estão mais difíceis de interpretar se baseando apenas nos gráficos da ACF e PACF.
Vamos utilizar uma abordagem de ``gridsearch'' para tentar encontrar o modelo
mais simples que minimiza os critérios de informação.

## Critério de informação

Vamos realizar um ``gridsearch'' entre 0 e 3 para os parâmetros $p$ e $P$
(com exceção de alguns modelos específicos que tiveram problemas na otimização durante a estimação).

```{r, echo = F}
ARMA.res <- data.frame()
K = 3
for (p in 0:K) {
  for (P in 0:K) {
    if (!(p == 1 & P == 3) & !(p == 3 & P == 1)){
      mod <- Arima(y = dd_bc_auscafe, order = c(p, 0, 1), seasonal = c(P, 0, 1))
      ARMA.res <- rbind(ARMA.res, c(p, P, mod$aic, mod$bic, mod$aicc))
    }
  }
}
names(ARMA.res) = c('p', 'P','AIC', 'BIC', 'AICc')
```


```{r, echo = F, message=F}
library(knitr)
kable(ARMA.res)
```

Verificamos que minimizamos AIC e AICc com $p = 2$, $P = 3$, enquanto minimizamos o BIC com $p = P = 0$.
Vamos trabalhar ambos os modelos.

## Estimação

Vamos utilizar a função `Arima` para ajustar os modelos SARIMA(2, 1, 1)(3, 1, 1)[12] e SARIMA(0, 1, 1)(0, 1, 1)[12], com transformação de Box-Cox (automática).

```{r, echo=F}
mod1 <- Arima(auscafe, order = c(2, 1, 1), seasonal = c(3, 1, 1), lambda = "auto")
summary(mod1)
```

```{r, echo=F}
mod2 <- Arima(auscafe, order = c(0, 1, 1), seasonal = c(0, 1, 1), lambda = "auto")
summary(mod2)
```

Vale notar que, por mais que estamos deixando a função escolher o parâmetro `lambda`, ele escolhe o mesmo que encontramos
anteriormente. (Existe uma vantagem em deixar a função fazer isso: o modelo todo é estimado com uma função só,
então fica trivial de utilizar funções de previsão que automaticamente calculam intervalos preditivos,
por exemplo.)

## Diagnóstico

Precisamos verificar os resíduos das nossas estimações. Fazemos isso com a função `checkresiduals`, e também aplicamos
um teste de Jarque-Bera (este tem hipótese nula de assimetria 0 e excesso de curtose 3).

```{r, echo=F}
checkresiduals(mod1)
jarque.bera.test(mod1$residuals)
```

```{r, echo=F}
checkresiduals(mod2)
jarque.bera.test(mod2$residuals)
```

Vamos discutir os resultados obtidos. O modelo SARIMA(2, 1, 1)(3, 1, 1)[12] rejeita as observações serem i.i.d., porém rejeita também a normalidade. Observamos valores extremos nos resíduos, assim como spike em lag 13.
O modelo SARIMA(0, 1, 1)(0, 1, 1)[12] rejeita as observações serem i.i.d., mas não rejeita a normalidade!
Vemos que várias autocorrelações estão fora do intervalo de confiança.

Com a posse de todas essas informações, acabamos por escolher o modelo SARIMA(0, 1, 1)(0, 1, 1)[12], que, mesmo que não seja
perfeito, é um dos melhores no ajuste aos dados e também é o mais simples entre os dois.
Observe ainda que este modelo é também escolhido pela função `auto.arima`:

```{r, echo = F}
auto.arima(auscafe)
```

