---
title: "Identificação de Modelos ARIMA"
author: "Lucas Resck e Lucas Moschen"
date: \today
output:
  pdf_document:
    template: null
bibliography: references.bib
# link-citations: yes
---

1. (Questão 30 - 5) Considere o modelo abaixo.

$$
Z_t = \sum_{j=0}^m \beta_j t^j  + \frac{\theta(B)}{\phi(B)\Delta^d}a_t
$$
Prove que, se $m > d$:

a. Tomando-se $d$ diferenças,  obtemos um modelo não estacionário, com uma tendência polinomial de grau $m-d = h$. 

Temos que $\Delta^d Z_t = (1 - B)^d Z_t = \sum_{k=0}^d {d\choose k}(-1)^kB^k Z_t$, usando o binômio de Newton. Em particular: 
$$
\begin{split}
\Delta^d t^j &= \sum_{k=0}^d {d\choose k}(-1)^kB^k t^j \\
&= \sum_{k=0}^d {d\choose k}(-1)^k(t-k)^j \\
&= \sum_{k=0}^d {d\choose k}(-1)^k\sum_{i=0}^j{j\choose i}t^ik^{j-i}(-1)^{j-i} \\
&= \sum_{i=0}^j {j \choose i}t^i(-1)^{j-i}\sum_{k=0}^d {d \choose k}(-1)^{d-k}(d-k)^{j-i}, \text{ redefinindo } k = d-k \\
&= \sum_{i=0}^j {j \choose i}t^i(-1)^{j-i}\sum_{k=0}^d {d \choose k}(-1)^{d+k}(d-k)^{j-i} \\
&= \sum_{i=0}^j {j \choose i}t^i(-1)^{j-i}(-1)^d\sum_{k=0}^d {d \choose k}(-1)^k(d-k)^{j-i} \\
&= \sum_{i=0}^j {j \choose i}t^i(-1)^{j-i}d!(-1)^d\begin{Bmatrix}j-i \\ d\end{Bmatrix}, \text{ onde o último é o número de Stirling de 2º tipo}
\end{split}
$$


Sabemos que se $d > j - i \implies i > j - d$, o número de Stirling é $0$, isto é, $\Delta^d t^j$ é um polinômio de ordem $(j - d)^+$. Em particular, se $d > j$, teremos um polinômio de ordem $0$. 

<!---
Poderíamos fazer de uma maneira alternativa: provar, por indução em d que \Delta^d t^j é um polinômio de ordem (j - d)^+
-->

$$
\Delta^d Z_t = \sum_{j=0}^m \beta_j \Delta^d t^j + \frac{\theta(B)}{\phi(B)}a_t
$$
Defina $m -d = h > 0$. Quando $j=m$, teremos que $\Delta^d t^m$ será um polinômio de ordem $h$, pelo que vimos acima, dado que o número de Stirling é diferente de $0$.   

b. Tomando-se $m$ diferenças obteremos um processo estacionário não invertível. 

Tomando $m$ diferenças, teremos um polinômio de ordem $0$. Seja, então, se olharmos a forma acima, o único valor diferente de $0$ será quando $i=0, j = d = m$. 

$$
\Delta^m Z_t = \beta_m m! + \frac{\theta(B)}{\phi(B)}a_t
$$

Esse processo não é invertível porque não podemos escrever $a_t = \pi(B)Z_t$, dado à presença do nível. 

2. (Questão 31 - 5) Prove que se $W_t = (1 - B)Z_t$, então $Z_t = W_t + W_{t-1} + ...$

Podemos representar $\frac{1}{1 - B}$ como: 

$$
\frac{1}{1 - B} = \sum_{j=0}^{\infty}B^j,
$$
portanto $Z_t = \sum_{j=0}^{\infty}B^jW_t = W_t + W_{t-1} + W_{t-2} + ...$.

3. (Questão 32 - 5) Prove que, na forma invertida do modelo, $\sum_{j=1}^{\infty} \pi_j = 1$.

Na forma invertida do modelo, ele é escrito da seguinte forma:

$$\pi(B) Z_t = a_t$$

Vamos expandir o polinômio e tomar o valor esperado:

$$
\begin{split}
  a_t &= \left[1 - \sum_{j=1}^\infty \pi_j B^j \right] Z_t \\
  a_t &= Z_t - \pi_1Z_{t-1} - \pi_2 Z_{t-2} - \cdots \\
  \mathbb{E}\{a_t\} &= \mathbb{E}\{Z_t - \pi_1 Z_{t-1} - \pi_2 Z_{t-2} - \cdots\} \\
  0 &= \left(1 - \sum_{j=1}^\infty \pi_j\right) \mu \\
  0 &= 1 - \sum_{j=1}^\infty \pi_j \\
  \sum_{j=1}^\infty \pi_j &= 1\\
\end{split}
$$

Observe que assumimos que $\mu \ne 0$. Isso na verdade é sem perda de generalidade, afinal:

$$
  \begin{split}
    \pi(B) Z_t &= a_t \\
    \theta(B) \pi(B) Z_t &= \theta(B) a_t \\
    \varphi(B) Z_t &= \theta(B) a_t \\
    \phi(B) \Delta^dZ_t &= \theta(B) a_t \\
    \phi(B) \Delta^d(Z_t + \alpha) &= \theta(B) a_t \\
    \phi(B) \Delta^dY_t &= \theta(B) a_t \\
    &\vdots \\
    \pi(B)Y_t &= a_t \\
    \pi(B)(Z_t + \alpha) &= a_t \\
  \end{split}
$$

5. (Questão 1 - 6) Prove que se $\rho_j = \phi^{|j|}, |\phi| < 1$, então

$$
Var(r_j) = \frac{1}{N}\left[\frac{(1 + \phi^2)(1 - \phi^{2j})}{1-\phi^2} - 2j\phi^{2j}\right], 
$$
em particular $Var(r_1) = \frac{1}{N}(1 - \phi^2)$. 

Observe que, se vale que $\rho_j = \phi^{|j|}$, então essas correlações são do modelo AR(1):

$$Z_t = \phi Z_{t-1} + a_t$$

Ora, @fuller2009introduction mostrou que, nesse caso,

$$\textrm{Var}(r_j) = \dfrac{N-j}{N^2}\left[\dfrac{(1+\phi^2)(1-\phi^{2j})}{1-\phi^2} - 2j\phi^{2j}\right]$$

Acreditamos que a demonstração está fora do escopo deste exercício, pois envolve vários teoremas e bastante manipulações algébricas.

----

```{r, echo = F, warning=F, message=F}
library(tseries)
library(forecast)
library(aTSA)
```

2. Simulação da distribuição da estatística de teste de Dickey-Fuller
Acesse o livro 'Econometria de Séries Temporais' 2a. Edição do Rodrigo de Losso Bueno na "Minha Biblioteca"
Leia Cap 4.5.1, 4.5.2 e 4.5.3, Pag 116 [(ou 134 online)](https://integrada.minhabiblioteca.com.br/#/books/9788522128259/pageid/134)

```{r}
df.simulation <- function(n, Tr, S, e.sd, phi, intercept = 0){
  t.stat <- c(1:S)
  for(i in 1:S){
    # Passo 1
    e <- rnorm(n + Tr, mean = 0, sd = e.sd)
    # Passo 2
    y <- e + intercept
    for(j in (length(phi)+1):(n+Tr)){
      for(k in 1:length(phi)){
        y[j] <- phi[k]*y[(j-k)] + y[j]
      }
    }
    # Passo 3 e 4
    # x <- y[(n+1):(n+Tr)]
    # mom2 <- sum(x[1:(Tr-1)]*x[1:(Tr-1)])
    # rho.hat <- sum(x[1:(Tr-1)]*x[2:Tr])/mom2
    # Sn <- sum((x[2:Tr] - rho.hat*x[1:(Tr-1)])^2)/(Tr-1)
    #
    # t.stat[i] = (rho.hat - 1)*sqrt(mom2)/Sn

    test <- tseries::adf.test(y[(n+1):(n+Tr)], alternative = "stationary")
    if(intercept == 0){
     t.stat[i] = test$statistic
    }
    else{
     t.stat[i] = test$type2[2, 'ADF']
    }
  }
  return(t.stat)
}
```

Primeiro, para fins de comparação, vejamos a distribuição quando $\phi = 1$.

```{r, warning=F}
set.seed(10000)

n <- 50
Tr <- 100
S <- 1000
e.sd <- 1
phi <- c(1)

t.stat <- df.simulation(n,Tr,S,e.sd,phi)
hist(t.stat, breaks = 50 , freq = F, col = 'grey',
      main = 'Densidade da t-estatística')
```

Vemos que o gráfico é similar com o exemplo do livro, o que é um *sanity check* interessante.

2.1 Simule os valores críticos da estatística de teste DF, como proposto no Cap 4.5.1, após eq (2) com $\phi_1 = 0.8$.

```{r, warning=F}
phi <- c(0.8)

t.stat <- df.simulation(n,Tr,S,e.sd,phi)
hist(t.stat, breaks = 50 , freq = F, col = 'grey',
      main = 'Densidade da t-estatística')
```

2.2 Repita a simulação, mas agora adicione termos autoregressivos no modelo, verifique que a distribuição da estatística de teste permanece inalterada.

```{r, warning = F}
phi <- c(0.8,-0.4, 0.2)

t.stat <- df.simulation(n,Tr,S,e.sd,phi)
hist(t.stat, breaks = 50 , freq = F, col = 'grey',
      main = 'Densidade da t-estatística', )
```

De fato as distribuições são bem similares, como podemos observar pelos gráficos.

2.3 Adicione intercepto e verifique se a nova distribuição da estatística de teste muda.

```{r, warning = F}
phi <- c(0.8,-0.4, 0.2)

t.stat <- df.simulation(n,Tr,S,e.sd,phi, intercept = 100)

hist(t.stat, breaks = 50 , freq = F, col = 'grey',
      main = 'Densidade da t-estatística')
```

A distribuição também aparenta ser a mesma das anteriores!

---

3. Identificação do modelo: Identifique o modelo ARIMA para a série de dados de inflação

Primeiro podemos verificar com que tipo de série estamos lidando através de um plot do gráfico. Os dados são de caráter mensal e se inicia em agosto de 1994 até agosto de 2020. Não há dados faltantes nem duplicados.

```{r, echo = F}
ipca <- read.csv('ipca.csv')
ipca <- ts(ipca$ipca.acum, start = c(1994,8), end = c(2020,8), frequency = 12)
plot(ipca, main = 'Ipca acumulado', xlab = 'Ano')
```

Vejamos se é necessário uma transformação para estabilizar a variância. O que faremos é dividir a nossa série temporal em um conjunto de 8 observações consecutivas, e estimar a média e a amplitude de cada um.

```{r}
x = c()
y = c()
N = length(ipca)
M = ceiling(N/8)
for (i in 0:(M-1)) {
  begin = ceiling(N/M)*i
  end = min(c(ceiling(N/M)*(i+1)-1, N))
  x = c(x, mean(ipca[begin:end]))
  y = c(y, max(ipca[begin:end]) - min(ipca[begin:end]))
}
plot(x, y, main="Média vs. amplitude", xlab = "Média", ylab = "Amplitude")
```

É difícil de tirar grandes conclusões a partir desse gráfico, mas podemos supor que a média cresce linearmente com a amplitude. Sendo assim, sabemos que a transformação indicada é a logarítmica.

```{r}
ipca.log = log(ipca)
plot(ipca.log, main = 'Transformação do Ipca pelo log', xlab = 'Ano', ylab = 'log')
```

Está claro que essa série tem uma tendência, pelo fato de estarmos tomando ipca acumulado. Vamos capturar o valor mensal, portanto:

```{r, echo = F}
ipca.month <- diff(ipca.log)
plot(ipca.month, main = 'Ipca mensal', xlab = 'Ano', ylab = 'log %')
```

Checando o Teste Dickey-Fuller aumentado, temos que o p-valor é menor do que 0.01, o que faz com que rejeitemos a hipótese de que a série é não estacionária. Concluímos que esse modelo já pode ser utilizado para identificar os parâmetros $p$ e $q$.

```{r, warning=F, mensage = F}
tseries::adf.test(ipca.month, alternative = "stationary")
```

Antes disso, vejamos a ACF e a PACF

```{r, warning=F, echo = F}
par(mfrow = c(1,2))
acf(ipca.month, lag.max = 24)
pacf(ipca.month, lag.max = 24)
```

Olhando o gráfico fica difícil discernir. Mas é aparente que a PACF morre logo nos primeiros valores. Portanto, apenas olhando os gráficos, chegaríamos em um modelo AR(p). Em contrapartida, vamos ver como se comportam os critérios de informação. Vamos obter em um dataframe o AIC, BIC e AICC. Estamos tomando $K = L = log(N)$ como valor máximo.

```{r, echo = F}
ARMA.res <- data.frame()
## valor máximo de p,q.
K <- trunc(log(length(ipca.month)))
L <- K
for (p in 0:K) {
    for (q in 0:K) {
        model <- Arima(y = ipca.month, order = c(p, 0, q))
        ARMA.res <- rbind(ARMA.res, c(p,q,model$aic, model$bic, model$aicc))
    }
}
names(ARMA.res) = c('p', 'q','AIC', 'BIC', 'AICC')
```

```{r, echo = F, message=F}
library(knitr)
kable(ARMA.res)
```

Os critérios AIC e AICc tem informação mínima quandp $p = 3, q = 5$, o modelo com ordem alta (como já era esperado). Enquanto isso, o BIC subestima a ordem do modelo com $p = 1, q = 0$, o que corrobora os gráficos ACF e PACF.

O seguinte gráfico apenas ajuda a identificar quais os índices do dataframe que contêm os parâmetros que minimizam as informações.

```{r, echo = F}
plot(ARMA.res$AIC, type = 'l', col = 'green',
     xlab = 'Índice do Dataframe', ylab = 'Informação')
lines(ARMA.res$BIC, col = 'red')
lines(ARMA.res$AICC, col = 'blue')
legend(x = 30, y = -2450, legend = c('AIC', 'BIC','AICc'), fill = c('green', 'red', 'blue'))
```

Agora nós vamos ajustar os dois modelos encontrados aos dados e analisar os resíduos.

```{r}
model = arima(ipca.month, order = c(3, 1, 5))
checkresiduals(model)
```

```{r}
model = arima(ipca.month, order = c(1, 1, 0))
checkresiduals(model)
```

Os resultados do teste de Ljung-Box indicam que ambos os modelos não possuem resíduos não correlacionados. Porém, o p-valor do modelo ARIMA(1, 1, 0) é menor, o que indica um melhor ajuste do modelo ARIMA(3, 1, 5).

# Referências
