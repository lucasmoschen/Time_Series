---
title: "Modelagem ARIMA e Análise de resíduos"
author: "Lucas Resck e Lucas Moschen"
date: \today
output:
  pdf_document:
    template: null
bibliography: references.bib
link-citations: yes
---

```{r load_libraries, echo=F, message=F, warning=F }
library(fpp2)
library(forecast)
library(tseries)
library(qpcR)
```

# Método Box-Jenkins 

0. Tranformação dos dados para estabilizar a variância. 

1. Identificação

1.1 Checar a estacionaridade e diferencial $d$ vezes;

1.2 Visualizar autocorrelação e autocorrelação parcial dos dados;

1.3 Comparar informações AIC, BIC e AICc e selectionar $p$ e $q$.

2. Estimação 

2.1 Estimar os valores de $\phi$ e $\theta$ do modelo através de máxima verossimilhança.

3. Diagnóstico 

3.1 Visualizar os resíduos do fitting; 

3.2 Plotar histograma, autocorrelação e autocorrelação parcial dos resíduos; 

3.3 Testes de estacionaridade e de normalidade. 

# Assassinato de mulheres

Primeiro, vamos visualizar a série anual. Peecebemos uma clara tendência, com crescimento acentuado ao longo das décadas de 1960 e 1970, bem como um decréscimo após os anos de 1990. 

```{r, echo=F}
autoplot(wmurders, main = 'Total de mulheres assassinadas por 100 mil habitantes (EUA)',
               xlab = 'Tempo (anual)', ylab = 'Total')
```

## Transformação Box-Cox

Dada a nossa série, é importante que visualizemos a variância ao longo dela. Visualmente ela aparenta não ter variância constante, dado que no início da série, ela aparenta ser menor. Para isso vamos calcular a transformação ótima de Box-Cox. 

A transformação de Box-Cox é a seguinte, se $\{y_t\}$ for uma série temporal, 

$$
y_t^{(\lambda)} = \begin{cases} \frac{y_t^{\lambda} - 1}{\lambda}, \text{ se } \lambda \neq 0\\ \log(y_t),  \text{ se } \lambda = 0\end{cases}
$$

A escolha de $\lambda$ ótimo utiliza o método de Guerrero, que escolhe $\lambda$ que minimize o coeficiente de variação ($c_v = \frac{\sigma}{\mu}$) para subséries de $y_t$. A seguir podemos conferir o resultado da transformação. 

```{r, echo=F}
lambda <- BoxCox.lambda(wmurders)
bcwmurders <- BoxCox(wmurders, lambda)
autoplot(bcwmurders, 
         main = 'Total de mulheres assassinadas por 100 mil habitantes (EUA)',
         xlab = 'Tempo (anual)', ylab = 'Total') + 
annotate("text", x=1955, y=1.3, label= paste('lambda = ', round(lambda, digits = 3)))
```

## Identificação 

O próximo passo do método de Box-Jenkins é identificar os parâmetros $p, q, d$ do modelo ARIMA. O primeiro passo é reduzir para um modelo ARMA estacionário e, para isso, precisamos diferenciar o quanto foi necessário. Vamos utilizar o teste *Augmented Dickey-Fuller* para conferir a estacionariedade. 

### Teste de Estacionaridade ADF

O teste ADF veririfica se há presença de uma raíz unitária na série temporal. A hipótese nula é de que a série possui raíz unitária e é, portanto, não estacionária. Em contrapartida, a alternativa é que a série é estacionária.  Vamos considerar *p-valor* $< 0.05$ como evidência para rejeitar a hipótese nula, nesse e nos próximos testes que realizarmos. 

```{r,echo=F}
adf.test(bcwmurders, alternative = 'stationary')
```

Como $\text{p-valor} \ge 0.05$, não podemos rejeitar a hipótese nula de que a série é não estacionária. Em particular, inferimos que a série é não estacionária, e por isso vamos diferenciá-la uma vez. 

### Diferenciando 

Tomemos a primeira diferença e, assim, podemos refazer o teste ADF novamente. 

```{r, echo=F}
dwmurders <- diff(bcwmurders)
autoplot(dwmurders, main = 'Total de mulheres assassinadas por 100 mil habitantes (EUA)',
         xlab = 'Tempo (anual)', ylab = 'Total anual')
```

```{r,echo=F}
adf.test(dwmurders)
```

Notemos que nosso *p-valor* $< 0.05$ e, então, rejeitamos a hipótese nula. Podemos, portanto, seguir a identificação do modelo ARMA. 

### Autocorrelação e Autocorrelação Parcial 

Vamos visualizar o comportamento da ACF e PACF da série diferenciada. 

```{r, echo=F}
par(mfrow = c(1,2))
acf(dwmurders)
pacf(dwmurders)
```

Obtemos um pico no segundo lag, mas não podemos tomar nenhuma conclusão. Poderíamos afirmar, por exemplo, que ambos os gráficos morrem logo, o que indicaria um ruído branco. Mas para não falar isso de forma errônea, vamos considerar os critérios de informação. 

### Critérios de Informação 

Vamos utilizar os critérios de informação estudados: 

- Akaike Information Criterion (AIC)
- Akaike Information Criterion with Correction (AICc)
- Bayesian Information Criterion (BIC)

```{r, echo = F}
ARMA.res <- data.frame()
## valor máximo de p,q.
K <- trunc(log(length(dwmurders)))
L <- K
for (p in 0:K) {
    for (q in 0:K) {
        model <- Arima(y = dwmurders, order = c(p, 0, q))
        
        ARMA.res <- rbind(ARMA.res, c(p,q,model$aic, model$bic, model$aicc))
    }
}
names(ARMA.res) = c('p', 'q','AIC', 'BIC', 'AICC')
```

```{r, echo = F, message=F}
library(knitr)
kable(ARMA.res)
```

A partir da tabela, podemos checar alguns pontos: AIC e AICc são bem similares, então vamos ter uma escolha de modelo que respeite ambas as informações. Baseado nelas, o modelo ARMA(0,2) é o que minimiza as informações. Já o BIC é um pouco diferente e em particular ele seleciona o modelo ARMA(0,0) como já havíamos imaginado dados os gráficos. Porém vamos seguir com o modelo ARMA(0,2) dada a liberdade dos coeficientes que ganhamos. Podemos então estimar os coeficientes do modelo ARIMA(0,1,2).

## Estimação 

Para fazer a estimação, vamos utilizar a função `forecast::Arima` que permite fazer o processo de transformação que fizemos inicialmente com o mesmo método, faz a diferenciação e depois estima os coeficientes. Vejamos o sumário do modelo fittado. 

```{r, echo=F}
model <- Arima(wmurders, order = c(0,1,2), lambda = "auto")
summary(model)
```

agora temos a estimação do modelo e os correspondentes erro de treino. A partir desse modelo treinado, podemos fazer a diagnose do resíduos. 

## Diagnóstico 

Vamos utilizar a função `checkresiduals` para fazer a análise inicial dos resíduos gráfica e também utilizaremos os testes Ljung-Box e Jarque Bera.

O teste Ljung-Box testa se as $k-$correlações do modelo são nulas. Isto é, a hipótese nula é que os dados são distribuídos de forma descorrelacionada, enquanto a aleternativa é que alguma autocoorelação é não nula. 

Já o teste Jarque-Beta é sobre a assimetria e a kurtose da distribuição normal. A hipótese nula é que a assimetria é 0 e excesso de kurtose 0. 

```{r, echo=F}
checkresiduals(model)
jarque.bera.test(model$residuals)
```

A ACF dos resíduos é muito interessante, dados que todas estão entre as faixas de confiança de zero. Já é um indicativo de um processo estacionário, ruído branco. O histograma parece um pouco não simétrico, mas ainda sim a sua suavização tem uma cara de normal bem acentuada. Os resultados dos testes também são interessantes, dado que ambos não rejeitam as suas hipóteses nulas. Ou seja, não temos evidência para rejeitá-las. 

Assim estamos contentes com o modelo, e podemos fazer projeções. 

## Projeção 

Esse é o gráfico da projeção 3 passos a frente. 

```{r, echo=F}
forecast(model, h = 3) %>% autoplot()
```

# Uso de cartão de débito

Primeiro, vamos visualizar a série. Percebemos uma sutil sazonalidade, aparentemente anual, além de uma clara tendência. 

```{r, echo = F}
autoplot(debitcards, main = 'Uso de cartão de débito no varejo na Islândia',
               xlab = 'Tempo (mensal)', ylab = 'million ISK')
```

## Transformação Box-Cox

Pelo gráfico, parece que teremos que fazer alguma transformação de estabilidade da variância. Para isso, vamos utilizar a transformação Box-Cox, como explicada na série anterior. 

```{r, echo=F}
lambda <- BoxCox.lambda(debitcards)
bcdebitcards <- BoxCox(debitcards, lambda)
autoplot(bcdebitcards, main = 'Uso de cartão de débito no varejo na Islândia',
               xlab = 'Tempo (mensal)', ylab = 'million ISK') + 
annotate("text", x=2002, y=3.6, label= paste('lambda = ', round(lambda, digits = 3)))
```

## Identificação 

Agora, com a variância da série estabilizada, podemos fazer o teste de estacionaridade. Observe que o teste ADF possui possibilidade de tendência. Assim

### Teste de Estacionaridade ADF

```{r, echo=F}
adf.test(bcdebitcards)
```

Como $\text{p-valor} \ge 0.05$, não podemos rejeitar a hipótese nula de que a série é não estacionária. Portanto, vamos usar a primeira diferenciação.  

### Diferenciando 

```{r, echo=F}
ddebitcards <- diff(bcdebitcards)
autoplot(ddebitcards, main = 'Uso de cartão de débito no varejo na Islândia',
               xlab = 'Tempo (mensal)', ylab = 'million ISK mensal')
```

```{r}
adf.test(ddebitcards)
```

Assim, podemos rejeitar a hipótese nula, o que suporta a ideia de que a série é estacionária. Vamos considerar

### ACF e PACF

```{r, echo=F}
par(mfrow = c(1,2))
acf(ddebitcards)
pacf(ddebitcards)
```

Percebemos dois fatores bem destacados: uma grande correlação quando o $\text{Lag} = 12$, o que indica que existe uma sazonalidade anual; e que a PACF decresce exponencialmente, enquanto a ACF morre após $\text{Lag} = 1$, o que nos levaria a um modelo MA(1). Antes disso, vamos fazer uma diferenciação a cada 12 meses. 

```{r, echo=F}
ddebitcards <- diff(ddebitcards, 12)
autoplot(ddebitcards, main = 'Uso de cartão de débito no varejo na Islândia',
               xlab = 'Tempo (mensal)', ylab = 'million ISK mensal')
```

```{r, echo=F}
adf.test(ddebitcards)
```

Observe que ainda temos uma série estacionária. Vejamos a ACF e PACF novamente: 

```{r, echo=F}
par(mfrow = c(1,2))
acf(ddebitcards)
pacf(ddebitcards)
```

O pico diminuiu consideravelmente. Agora, sim, vamos considerar os critérios de informação para identificar o modelo.

### Critérios de Informação

```{r, echo = F}
ARMA.res <- data.frame()
## valor máximo de p,q.
K <- trunc(log(length(ddebitcards)))
L <- K
for (p in 0:K) {
    for (q in 0:K) {
        model <- Arima(y = ddebitcards, order = c(p, 0, q))
        ARMA.res <- rbind(ARMA.res, c(p,q,model$aic, model$bic, model$aicc))
    }
}
names(ARMA.res) = c('p', 'q','AIC', 'BIC', 'AICC')
```

```{r, echo = F, message=F}
library(knitr)
kable(ARMA.res)
```

Baseado no AIC e AICc, o modelo é ARMA(2,5). Baseado no BIC escolhemos um modelo bem mis simples, ARMA(2,0). Vou seguir com o modelo com mais parâmetros. 

## Estimação 

Estimamos os parâmetros do modelo, considerando uma diferenciação sazonal. 

```{r}
model <- Arima(debitcards, order = c(2,1,5), seasonal = c(0,1,0), lambda = "auto")
summary(model)
```

## Diagnóstico 

Aqui podemos conferir os resíduos. É importante detacar a aparência normal dos resíduos, o que é interessante. Porém a ACF ainda apresenta picos, o mais incomodativo é do período 12. 

O test Ljung-Box coorrobora esse fato, rejeitando a hipótese nula de que os coeficientes de correlação são iguais. Já Jarque Bera não rejeita a normalidade, como esperávamos. 

```{r}
checkresiduals(model)
jarque.bera.test(model$residuals)
```

O persistente pico pode ser um indicativo de que a série sazonal precise de coeficientes ARMA. Em particular, poderíamos colocar ARMA sazonal. Isso fica para as próximas análises. 

## Projeção 

Agora vamos conferir as projeções três passos a frente.

```{r}
forecast(model, h = 3) %>% autoplot()
```

# Comer fora na Austrália

Primeiro, vamos visualizar a série. 

```{r, echo=F}
autoplot(auscafe, main = 'Gastos mensais em comer fora na Austrália',
               xlab = 'Tempo (mensal)', ylab = 'Billion dolars')
```
